{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Stability of three algorithms for solving LS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will see how stable three different algorithms for solving the same least square problem are, with respect to the rounding off error. The theory to explain why some are more stable than another is quite involved and probably takes half of a semester to go through. If you are interested, the details are in the recommended text \"Numerical Linear Algebra\" by Trefethen and Bau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a LS problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to run the following command. I will give you extra credit though if you can explain why b is constructed in such a way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = 100\n",
    "n = 15\n",
    "t = np.arange(0,1+1.0/(m-1),1.0/(m-1)) # Set t to a discretization of [0,1]. len(t)=m\n",
    "A = np.array([t**i for i in range(n)]).T # Construct a submatrix of a Vandermonde matrix\n",
    "#size of A is m by n\n",
    "\n",
    "#truex is the real least square solutioin\n",
    "truex = np.array([-0.76913135,  0.46167844,  0.10294497, -0.55750683,  1.37792289,  1.1454379,\n",
    "   0.52179532, -2.59420408,  0.0355606,   1.67058624,  0.1212572,  -1.14884385,\n",
    "  -0.78537181, -1.18751783, 1])\n",
    "\n",
    "#Construct the right-hand side vector b so that least square solution of Ax=b is truex\n",
    "U,S,V = np.linalg.svd(A)\n",
    "AC = U[:,15:]\n",
    "b = A.dot(truex).reshape(100,1) + np.dot(AC, np.ones([85,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you will need to use three different algorithms that we talked about for solving LS. Use functions like solve, qr, svd... from the linalg package. \n",
    "\n",
    "In order to compare the solution with truex, we will just compare the last entry, which should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Via the normal equation\n",
    "\n",
    "(1) Directly use solve. No need to do cholesky (the solve function is doing that). \n",
    "\n",
    "Call the solution xn. You need to print running time (you can run a couple times to get a stable time), print the last entry difference: abs(xn[14]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000362873077393\n",
      "[ 0.83645053]\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "xn = np.linalg.solve(A.T.dot(A), A.T.dot(b))\n",
    "end = timeit.default_timer()\n",
    "print end - start\n",
    "print abs(xn[14]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Reduced QR\n",
    "\n",
    "(2) Solve LS via reduced QR. \n",
    "\n",
    "Call the solution xq. You need to print running time, print the last entry difference: abs(xq[14]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000691890716553\n",
      "[  2.90245961e-09]\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "Q,R = np.linalg.qr(A, mode='reduced')\n",
    "xq = np.linalg.solve(R, np.dot(Q.T, b))\n",
    "end = timeit.default_timer()\n",
    "print end - start\n",
    "print abs(xq[14]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reduced SVD\n",
    "\n",
    "(3) Solve LS via reduced QR. \n",
    "\n",
    "Call the solution xs. You need to print running time, print the last entry difference: abs(xs[14]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00121808052063\n",
      "[  8.52456616e-09]\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "U,S,V = np.linalg.svd(A, full_matrices=0)\n",
    "w = np.linalg.solve(np.diag(S), np.dot(U.T,b))\n",
    "xs = V.T.dot(w)\n",
    "end = timeit.default_timer()\n",
    "print end - start\n",
    "print abs(xs[14]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Write a paragraph (in this cell) commenting on stability and running time of each one:\n",
    "\n",
    "Your exact outputs in (1)(2)(3) depend on your computer. But generally, \n",
    "\n",
    "efficiency-wise (speed): normal equation is the fastest, QR second, and SVD is the slowest. The QR method is not much slower than the normal equation method.\n",
    "\n",
    "stability-wise: normal equation is REALLY BAD. QR and SVD is about the same.\n",
    "\n",
    "Overall, using QR decomposition is the \"daily choice\" for solving a least square problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
